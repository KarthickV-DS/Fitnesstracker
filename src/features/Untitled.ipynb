{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236e66a7-850f-4a2a-8a5a-36a884628f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#                                                            #\n",
    "#    Mark Hoogendoorn and Burkhardt Funk (2017)              #\n",
    "#    Machine Learning for the Quantified Self                #\n",
    "#    Springer                                                #\n",
    "#    Chapter 7                                               #\n",
    "#                                                            #\n",
    "##############################################################\n",
    "\n",
    "# Updated by Dave Ebbelaar on 12-01-2023\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "class ClassificationAlgorithms:\n",
    "\n",
    "    # Forward selection for classification which selects a pre-defined number of features (max_features)\n",
    "    # that show the best accuracy. We assume a decision tree learning for this purpose, but\n",
    "    # this can easily be changed. It return the best features.\n",
    "    def forward_selection(self, max_features, X_train, y_train):\n",
    "        # Start with no features.\n",
    "        ordered_features = []\n",
    "        ordered_scores = []\n",
    "        selected_features = []\n",
    "        ca = ClassificationAlgorithms()\n",
    "        prev_best_perf = 0\n",
    "\n",
    "        # Select the appropriate number of features.\n",
    "        for i in range(0, max_features):\n",
    "            print(i)\n",
    "\n",
    "            # Determine the features left to select.\n",
    "            features_left = list(set(X_train.columns) - set(selected_features))\n",
    "            best_perf = 0\n",
    "            best_attribute = \"\"\n",
    "\n",
    "            # For all features we can still select...\n",
    "            for f in features_left:\n",
    "                temp_selected_features = copy.deepcopy(selected_features)\n",
    "                temp_selected_features.append(f)\n",
    "\n",
    "                # Determine the accuracy of a decision tree learner if we were to add\n",
    "                # the feature.\n",
    "                (\n",
    "                    pred_y_train,\n",
    "                    pred_y_test,\n",
    "                    prob_training_y,\n",
    "                    prob_test_y,\n",
    "                ) = ca.decision_tree(\n",
    "                    X_train[temp_selected_features],\n",
    "                    y_train,\n",
    "                    X_train[temp_selected_features],\n",
    "                )\n",
    "                perf = accuracy_score(y_train, pred_y_train)\n",
    "\n",
    "                # If the performance is better than what we have seen so far (we aim for high accuracy)\n",
    "                # we set the current feature to the best feature and the same for the best performance.\n",
    "                if perf > best_perf:\n",
    "                    best_perf = perf\n",
    "                    best_feature = f\n",
    "            # We select the feature with the best performance.\n",
    "            selected_features.append(best_feature)\n",
    "            prev_best_perf = best_perf\n",
    "            ordered_features.append(best_feature)\n",
    "            ordered_scores.append(best_perf)\n",
    "        return selected_features, ordered_features, ordered_scores\n",
    "\n",
    "    # Apply a neural network for classification upon the training data (with the specified composition of\n",
    "    # hidden layers and number of iterations), and use the created network to predict the outcome for both the\n",
    "    # test and training set. It returns the categorical predictions for the training and test set as well as the\n",
    "    # probabilities associated with each class, each class being represented as a column in the data frame.\n",
    "    def feedforward_neural_network(\n",
    "        self,\n",
    "        train_X,\n",
    "        train_y,\n",
    "        test_X,\n",
    "        hidden_layer_sizes=(100,),\n",
    "        max_iter=2000,\n",
    "        activation=\"logistic\",\n",
    "        alpha=0.0001,\n",
    "        learning_rate=\"adaptive\",\n",
    "        gridsearch=True,\n",
    "        print_model_details=False,\n",
    "    ):\n",
    "\n",
    "        if gridsearch:\n",
    "            tuned_parameters = [\n",
    "                {\n",
    "                    \"hidden_layer_sizes\": [\n",
    "                        (5,),\n",
    "                        (10,),\n",
    "                        (25,),\n",
    "                        (100,),\n",
    "                        (\n",
    "                            100,\n",
    "                            5,\n",
    "                        ),\n",
    "                        (\n",
    "                            100,\n",
    "                            10,\n",
    "                        ),\n",
    "                    ],\n",
    "                    \"activation\": [activation],\n",
    "                    \"learning_rate\": [learning_rate],\n",
    "                    \"max_iter\": [1000, 2000],\n",
    "                    \"alpha\": [alpha],\n",
    "                }\n",
    "            ]\n",
    "            nn = GridSearchCV(\n",
    "                MLPClassifier(), tuned_parameters, cv=5, scoring=\"accuracy\"\n",
    "            )\n",
    "        else:\n",
    "            # Create the model\n",
    "            nn = MLPClassifier(\n",
    "                hidden_layer_sizes=hidden_layer_sizes,\n",
    "                activation=activation,\n",
    "                max_iter=max_iter,\n",
    "                learning_rate=learning_rate,\n",
    "                alpha=alpha,\n",
    "            )\n",
    "\n",
    "        # Fit the model\n",
    "        nn.fit(\n",
    "            train_X,\n",
    "            train_y.values.ravel(),\n",
    "        )\n",
    "\n",
    "        if gridsearch and print_model_details:\n",
    "            print(nn.best_params_)\n",
    "\n",
    "        if gridsearch:\n",
    "            nn = nn.best_estimator_\n",
    "\n",
    "        # Apply the model\n",
    "        pred_prob_training_y = nn.predict_proba(train_X)\n",
    "        pred_prob_test_y = nn.predict_proba(test_X)\n",
    "        pred_training_y = nn.predict(train_X)\n",
    "        pred_test_y = nn.predict(test_X)\n",
    "        frame_prob_training_y = pd.DataFrame(pred_prob_training_y, columns=nn.classes_)\n",
    "        frame_prob_test_y = pd.DataFrame(pred_prob_test_y, columns=nn.classes_)\n",
    "\n",
    "        return pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y\n",
    "\n",
    "    # Apply a support vector machine for classification upon the training data (with the specified value for\n",
    "    # C, epsilon and the kernel function), and use the created model to predict the outcome for both the\n",
    "    # test and training set. It returns the categorical predictions for the training and test set as well as the\n",
    "    # probabilities associated with each class, each class being represented as a column in the data frame.\n",
    "    def support_vector_machine_with_kernel(\n",
    "        self,\n",
    "        train_X,\n",
    "        train_y,\n",
    "        test_X,\n",
    "        kernel=\"rbf\",\n",
    "        C=1,\n",
    "        gamma=1e-3,\n",
    "        gridsearch=True,\n",
    "        print_model_details=False,\n",
    "    ):\n",
    "        # Create the model\n",
    "        if gridsearch:\n",
    "            tuned_parameters = [\n",
    "                {\"kernel\": [\"rbf\", \"poly\"], \"gamma\": [1e-3, 1e-4], \"C\": [1, 10, 100]}\n",
    "            ]\n",
    "            svm = GridSearchCV(\n",
    "                SVC(probability=True), tuned_parameters, cv=5, scoring=\"accuracy\"\n",
    "            )\n",
    "        else:\n",
    "            svm = SVC(\n",
    "                C=C, kernel=kernel, gamma=gamma, probability=True, cache_size=7000\n",
    "            )\n",
    "\n",
    "        # Fit the model\n",
    "        svm.fit(train_X, train_y.values.ravel())\n",
    "\n",
    "        if gridsearch and print_model_details:\n",
    "            print(svm.best_params_)\n",
    "\n",
    "        if gridsearch:\n",
    "            svm = svm.best_estimator_\n",
    "\n",
    "        # Apply the model\n",
    "        pred_prob_training_y = svm.predict_proba(train_X)\n",
    "        pred_prob_test_y = svm.predict_proba(test_X)\n",
    "        pred_training_y = svm.predict(train_X)\n",
    "        pred_test_y = svm.predict(test_X)\n",
    "        frame_prob_training_y = pd.DataFrame(pred_prob_training_y, columns=svm.classes_)\n",
    "        frame_prob_test_y = pd.DataFrame(pred_prob_test_y, columns=svm.classes_)\n",
    "\n",
    "        return pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y\n",
    "\n",
    "    # Apply a support vector machine for classification upon the training data (with the specified value for\n",
    "    # C, epsilon and the kernel function), and use the created model to predict the outcome for both the\n",
    "    # test and training set. It returns the categorical predictions for the training and test set as well as the\n",
    "    # probabilities associated with each class, each class being represented as a column in the data frame.\n",
    "    def support_vector_machine_without_kernel(\n",
    "        self,\n",
    "        train_X,\n",
    "        train_y,\n",
    "        test_X,\n",
    "        C=1,\n",
    "        tol=1e-3,\n",
    "        max_iter=1000,\n",
    "        gridsearch=True,\n",
    "        print_model_details=False,\n",
    "    ):\n",
    "        # Create the model\n",
    "        if gridsearch:\n",
    "            tuned_parameters = [\n",
    "                {\"max_iter\": [1000, 2000], \"tol\": [1e-3, 1e-4], \"C\": [1, 10, 100]}\n",
    "            ]\n",
    "            svm = GridSearchCV(LinearSVC(), tuned_parameters, cv=5, scoring=\"accuracy\")\n",
    "        else:\n",
    "            svm = LinearSVC(C=C, tol=tol, max_iter=max_iter)\n",
    "\n",
    "        # Fit the model\n",
    "        svm.fit(train_X, train_y.values.ravel())\n",
    "\n",
    "        if gridsearch and print_model_details:\n",
    "            print(svm.best_params_)\n",
    "\n",
    "        if gridsearch:\n",
    "            svm = svm.best_estimator_\n",
    "\n",
    "        # Apply the model\n",
    "\n",
    "        distance_training_platt = 1 / (1 + np.exp(svm.decision_function(train_X)))\n",
    "        pred_prob_training_y = (\n",
    "            distance_training_platt / distance_training_platt.sum(axis=1)[:, None]\n",
    "        )\n",
    "        distance_test_platt = 1 / (1 + np.exp(svm.decision_function(test_X)))\n",
    "        pred_prob_test_y = (\n",
    "            distance_test_platt / distance_test_platt.sum(axis=1)[:, None]\n",
    "        )\n",
    "        pred_training_y = svm.predict(train_X)\n",
    "        pred_test_y = svm.predict(test_X)\n",
    "        frame_prob_training_y = pd.DataFrame(pred_prob_training_y, columns=svm.classes_)\n",
    "        frame_prob_test_y = pd.DataFrame(pred_prob_test_y, columns=svm.classes_)\n",
    "\n",
    "        return pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y\n",
    "\n",
    "    # Apply a nearest neighbor approach for classification upon the training data (with the specified value for\n",
    "    # k), and use the created model to predict the outcome for both the\n",
    "    # test and training set. It returns the categorical predictions for the training and test set as well as the\n",
    "    # probabilities associated with each class, each class being represented as a column in the data frame.\n",
    "    def k_nearest_neighbor(\n",
    "        self,\n",
    "        train_X,\n",
    "        train_y,\n",
    "        test_X,\n",
    "        n_neighbors=5,\n",
    "        gridsearch=True,\n",
    "        print_model_details=False,\n",
    "    ):\n",
    "        # Create the model\n",
    "        if gridsearch:\n",
    "            tuned_parameters = [{\"n_neighbors\": [1, 2, 5, 10]}]\n",
    "            knn = GridSearchCV(\n",
    "                KNeighborsClassifier(), tuned_parameters, cv=5, scoring=\"accuracy\"\n",
    "            )\n",
    "        else:\n",
    "            knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "\n",
    "        # Fit the model\n",
    "        knn.fit(train_X, train_y.values.ravel())\n",
    "\n",
    "        if gridsearch and print_model_details:\n",
    "            print(knn.best_params_)\n",
    "\n",
    "        if gridsearch:\n",
    "            knn = knn.best_estimator_\n",
    "\n",
    "        # Apply the model\n",
    "        pred_prob_training_y = knn.predict_proba(train_X)\n",
    "        pred_prob_test_y = knn.predict_proba(test_X)\n",
    "        pred_training_y = knn.predict(train_X)\n",
    "        pred_test_y = knn.predict(test_X)\n",
    "        frame_prob_training_y = pd.DataFrame(pred_prob_training_y, columns=knn.classes_)\n",
    "        frame_prob_test_y = pd.DataFrame(pred_prob_test_y, columns=knn.classes_)\n",
    "\n",
    "        return pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y\n",
    "\n",
    "    # Apply a decision tree approach for classification upon the training data (with the specified value for\n",
    "    # the minimum samples in the leaf, and the export path and files if print_model_details=True)\n",
    "    # and use the created model to predict the outcome for both the\n",
    "    # test and training set. It returns the categorical predictions for the training and test set as well as the\n",
    "    # probabilities associated with each class, each class being represented as a column in the data frame.\n",
    "    def decision_tree(\n",
    "        self,\n",
    "        train_X,\n",
    "        train_y,\n",
    "        test_X,\n",
    "        min_samples_leaf=50,\n",
    "        criterion=\"gini\",\n",
    "        print_model_details=False,\n",
    "        export_tree_path=\"Example_graphs/Chapter7/\",\n",
    "        export_tree_name=\"tree.dot\",\n",
    "        gridsearch=True,\n",
    "    ):\n",
    "        # Create the model\n",
    "        if gridsearch:\n",
    "            tuned_parameters = [\n",
    "                {\n",
    "                    \"min_samples_leaf\": [2, 10, 50, 100, 200],\n",
    "                    \"criterion\": [\"gini\", \"entropy\"],\n",
    "                }\n",
    "            ]\n",
    "            dtree = GridSearchCV(\n",
    "                DecisionTreeClassifier(), tuned_parameters, cv=5, scoring=\"accuracy\"\n",
    "            )\n",
    "        else:\n",
    "            dtree = DecisionTreeClassifier(\n",
    "                min_samples_leaf=min_samples_leaf, criterion=criterion\n",
    "            )\n",
    "\n",
    "        # Fit the model\n",
    "\n",
    "        dtree.fit(train_X, train_y.values.ravel())\n",
    "\n",
    "        if gridsearch and print_model_details:\n",
    "            print(dtree.best_params_)\n",
    "\n",
    "        if gridsearch:\n",
    "            dtree = dtree.best_estimator_\n",
    "\n",
    "        # Apply the model\n",
    "        pred_prob_training_y = dtree.predict_proba(train_X)\n",
    "        pred_prob_test_y = dtree.predict_proba(test_X)\n",
    "        pred_training_y = dtree.predict(train_X)\n",
    "        pred_test_y = dtree.predict(test_X)\n",
    "        frame_prob_training_y = pd.DataFrame(\n",
    "            pred_prob_training_y, columns=dtree.classes_\n",
    "        )\n",
    "        frame_prob_test_y = pd.DataFrame(pred_prob_test_y, columns=dtree.classes_)\n",
    "\n",
    "        if print_model_details:\n",
    "            ordered_indices = [\n",
    "                i[0]\n",
    "                for i in sorted(\n",
    "                    enumerate(dtree.feature_importances_),\n",
    "                    key=lambda x: x[1],\n",
    "                    reverse=True,\n",
    "                )\n",
    "            ]\n",
    "            print(\"Feature importance decision tree:\")\n",
    "            for i in range(0, len(dtree.feature_importances_)):\n",
    "                print(\n",
    "                    train_X.columns[ordered_indices[i]],\n",
    "                )\n",
    "                print(\n",
    "                    \" & \",\n",
    "                )\n",
    "                print(dtree.feature_importances_[ordered_indices[i]])\n",
    "            tree.export_graphviz(\n",
    "                dtree,\n",
    "                out_file=export_tree_path + export_tree_name,\n",
    "                feature_names=train_X.columns,\n",
    "                class_names=dtree.classes_,\n",
    "            )\n",
    "\n",
    "        return pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y\n",
    "\n",
    "    # Apply a naive bayes approach for classification upon the training data\n",
    "    # and use the created model to predict the outcome for both the\n",
    "    # test and training set. It returns the categorical predictions for the training and test set as well as the\n",
    "    # probabilities associated with each class, each class being represented as a column in the data frame.\n",
    "    def naive_bayes(self, train_X, train_y, test_X):\n",
    "        # Create the model\n",
    "        nb = GaussianNB()\n",
    "\n",
    "        # Fit the model\n",
    "        nb.fit(train_X, train_y)\n",
    "\n",
    "        # Apply the model\n",
    "        pred_prob_training_y = nb.predict_proba(train_X)\n",
    "        pred_prob_test_y = nb.predict_proba(test_X)\n",
    "        pred_training_y = nb.predict(train_X)\n",
    "        pred_test_y = nb.predict(test_X)\n",
    "        frame_prob_training_y = pd.DataFrame(pred_prob_training_y, columns=nb.classes_)\n",
    "        frame_prob_test_y = pd.DataFrame(pred_prob_test_y, columns=nb.classes_)\n",
    "\n",
    "        return pred_training_y, pred_test_y, frame_prob_training_y, frame_prob_test_y\n",
    "\n",
    "    # Apply a random forest approach for classification upon the training data (with the specified value for\n",
    "    # the minimum samples in the leaf, the number of trees, and if we should print some of the details of the\n",
    "    # model print_model_details=True) and use the created model to predict the outcome for both the\n",
    "    # test and training set. It returns the categorical predictions for the training and test set as well as the\n",
    "    # probabilities associated with each class, each class being represented as a column in the data frame.\n",
    "    def random_forest(\n",
    "        self,\n",
    "        train_X,\n",
    "        train_y,\n",
    "        test_X,\n",
    "        n_estimators=10,\n",
    "        min_samples_leaf=5,\n",
    "        criterion=\"gini\",\n",
    "        print_model_details=False,\n",
    "        gridsearch=True,\n",
    "    ):\n",
    "\n",
    "        if gridsearch:\n",
    "            tuned_parameters = [\n",
    "                {\n",
    "                    \"min_samples_leaf\": [2, 10, 50, 100, 200],\n",
    "                    \"n_estimators\": [10, 50, 100],\n",
    "                    \"criterion\": [\"gini\", \"entropy\"],\n",
    "                }\n",
    "            ]\n",
    "            rf = GridSearchCV(\n",
    "                RandomForestClassifier(), tuned_parameters, cv=5, scoring=\"accuracy\"\n",
    "            )\n",
    "        else:\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                criterion=criterion,\n",
    "            )\n",
    "\n",
    "        # Fit the model\n",
    "\n",
    "        rf.fit(train_X, train_y.values.ravel())\n",
    "\n",
    "        if gridsearch and print_model_details:\n",
    "            print(rf.best_params_)\n",
    "\n",
    "        if gridsearch:\n",
    "            rf = rf.best_estimator_\n",
    "\n",
    "        pred_prob_training_y = rf.predict_proba(train_X)\n",
    "        pred_prob_test_y = rf.predict_proba(test_X)\n",
    "        pred_training_y = rf.predict(train_X)\n",
    "        pred_test_y = rf.predict(test_X)\n",
    "        frame_prob_training_y = pd.DataFrame(pred_prob_training_y, columns=rf.classes_)\n",
    "        frame_prob_test_y = pd.DataFrame(pred_prob_test_y, columns=rf.classes_)\n",
    "\n",
    "        if print_model_details:\n",
    "            ordered_indices = [\n",
    "                i[0]\n",
    "                for i in sorted(\n",
    "                    enumerate(rf.feature_importances_), key=lambda x: x[1], reverse=True\n",
    "                )\n",
    "            ]\n",
    "            print(\"Feature importance random forest:\")\n",
    "            for i in range(0, len(rf.feature_importances_)):\n",
    "                print(\n",
    "                    train_X.columns[ordered_indices[i]],\n",
    "                )\n",
    "                print(\n",
    "                    \" & \",\n",
    "                )\n",
    "                print(rf.feature_importances_[ordered_indices[i]])\n",
    "\n",
    "        return (\n",
    "            pred_training_y,\n",
    "            pred_test_y,\n",
    "            frame_prob_training_y,\n",
    "            frame_prob_test_y,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5631d-1d96-4781-8ff6-53739e63c69d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
