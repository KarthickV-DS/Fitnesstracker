{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96987f91-9148-47e2-bb06-6f2994a30c3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from sklearn.neighbors import LocalOutlierFactor  # pip install scikit-learn\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Load data\n",
    "# --------------------------------------------------------------\n",
    "df = pd.read_pickle(\"data/processed/data_processed.pkl\")\n",
    "# --------------------------------------------------------------\n",
    "# Plotting outliers\n",
    "# --------------------------------------------------------------\n",
    "df.columns\n",
    "plt.rcParams['figure.figsize'] = [20,5]\n",
    "sns.boxplot(data = df , x=\"label\" , y=\"acceleration_x\",whis =1.5)\n",
    "outlier_columns = list(df.columns[:6])\n",
    "df[outlier_columns[:3]+[\"label\"]].boxplot(by=\"label\",figsize = (20,10),layout = (1,3))\n",
    "df[outlier_columns[3:]+[\"label\"]].boxplot(by=\"label\",figsize = (20,10),layout = (1,3))\n",
    "# --------------------------------------------------------------\n",
    "# Interquartile range (distribution based)\n",
    "# --------------------------------------------------------------\n",
    "def plot_binary_outliers(dataset, col, outlier_col, reset_index):\n",
    "    \"\"\" Plot outliers in case of a binary outlier score. Here, the col specifies the real data\n",
    "    column and outlier_col the columns with a binary value (outlier or not).\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The dataset\n",
    "        col (string): Column that you want to plot\n",
    "        outlier_col (string): Outlier column marked with true/false\n",
    "        reset_index (bool): whether to reset the index for plotting\n",
    "    \"\"\"\n",
    "\n",
    "    # Taken from: https://github.com/mhoogen/ML4QS/blob/master/Python3Code/util/VisualizeDataset.py\n",
    "\n",
    "    dataset = dataset.dropna(axis=0, subset=[col, outlier_col])\n",
    "    dataset[outlier_col] = dataset[outlier_col].astype(\"bool\")\n",
    "\n",
    "    if reset_index:\n",
    "        dataset = dataset.reset_index()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    plt.xlabel(\"samples\")\n",
    "    plt.ylabel(\"value\")\n",
    "\n",
    "    # Plot non outliers in default color\n",
    "    ax.plot(\n",
    "        dataset.index[~dataset[outlier_col]],\n",
    "        dataset[col][~dataset[outlier_col]],\n",
    "        \"+\",\n",
    "    )\n",
    "    # Plot data points that are outliers in red\n",
    "    ax.plot(\n",
    "        dataset.index[dataset[outlier_col]],\n",
    "        dataset[col][dataset[outlier_col]],\n",
    "        \"r+\",\n",
    "    )\n",
    "\n",
    "    plt.legend(\n",
    "        [\"outlier \" + col, \"no outlier \" + col],\n",
    "        loc=\"upper center\",\n",
    "        ncol=2,\n",
    "        fancybox=True,\n",
    "        shadow=True,\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def mark_outliers_iqr(dataset, col):\n",
    "    \"\"\"Function to mark values as outliers using the IQR method.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The dataset\n",
    "        col (string): The column you want apply outlier detection to\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original dataframe with an extra boolean column \n",
    "        indicating whether the value is an outlier or not.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = dataset.copy()\n",
    "\n",
    "    Q1 = dataset[col].quantile(0.25)\n",
    "    Q3 = dataset[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    dataset[col + \"_outlier\"] = (dataset[col] < lower_bound) | (\n",
    "        dataset[col] > upper_bound\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "# Insert IQR function\n",
    "\n",
    "\n",
    "# Plot a single column\n",
    "col = 'acceleration_x'\n",
    "dataset = mark_outliers_iqr(df , col)\n",
    "\n",
    "\n",
    "plot_binary_outliers(dataset,col,col+\"_outlier\",reset_index=True)\n",
    "# Loop over all columns\n",
    "for col in outlier_columns:\n",
    "    dataset = mark_outliers_iqr(df,col)\n",
    "    plot_binary_outliers(dataset,col,col+\"_outlier\",reset_index=True)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Chauvenets criteron (distribution based)\n",
    "# --------------------------------------------------------------\n",
    "df[outlier_columns[:3]+[\"label\"]].plot.hist(by=\"label\",figsize = (20,20),layout = (3,3))\n",
    "df[outlier_columns[3:]+[\"label\"]].plot.hist(by=\"label\",figsize = (20,20),layout = (3,3))\n",
    "# Check for normal distribution\n",
    "\n",
    "# Insert Chauvenet's function\n",
    "def mark_outliers_chauvenet(dataset, col, C=2):\n",
    "    \"\"\"Finds outliers in the specified column of datatable and adds a binary column with\n",
    "    the same name extended with '_outlier' that expresses the result per data point.\n",
    "    \n",
    "    Taken from: https://github.com/mhoogen/ML4QS/blob/master/Python3Code/Chapter3/OutlierDetection.py\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The dataset\n",
    "        col (string): The column you want apply outlier detection to\n",
    "        C (int, optional): Degree of certainty for the identification of outliers given the assumption \n",
    "                           of a normal distribution, typicaly between 1 - 10. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original dataframe with an extra boolean column \n",
    "        indicating whether the value is an outlier or not.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = dataset.copy()\n",
    "    # Compute the mean and standard deviation.\n",
    "    mean = dataset[col].mean()\n",
    "    std = dataset[col].std()\n",
    "    N = len(dataset.index)\n",
    "    criterion = 1.0 / (C * N)\n",
    "\n",
    "    # Consider the deviation for the data points.\n",
    "    deviation = abs(dataset[col] - mean) / std\n",
    "\n",
    "    # Express the upper and lower bounds.\n",
    "    low = -deviation / math.sqrt(C)\n",
    "    high = deviation / math.sqrt(C)\n",
    "    prob = []\n",
    "    mask = []\n",
    "\n",
    "    # Pass all rows in the dataset.\n",
    "    for i in range(0, len(dataset.index)):\n",
    "        # Determine the probability of observing the point\n",
    "        prob.append(\n",
    "            1.0 - 0.5 * (scipy.special.erf(high[i]) - scipy.special.erf(low[i]))\n",
    "        )\n",
    "        # And mark as an outlier when the probability is below our criterion.\n",
    "        mask.append(prob[i] < criterion)\n",
    "    dataset[col + \"_outlier\"] = mask\n",
    "    return dataset\n",
    "\n",
    "# Loop over all columns\n",
    "for col in outlier_columns:\n",
    "    dataset = mark_outliers_chauvenet(df,col)\n",
    "    plot_binary_outliers(dataset,col,col+\"_outlier\",reset_index=True)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Local outlier factor (distance based)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Insert LOF function\n",
    "def mark_outliers_lof(dataset, columns, n=20):\n",
    "    \"\"\"Mark values as outliers using LOF\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): The dataset\n",
    "        col (string): The column you want apply outlier detection to\n",
    "        n (int, optional): n_neighbors. Defaults to 20.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The original dataframe with an extra boolean column\n",
    "        indicating whether the value is an outlier or not.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = dataset.copy()\n",
    "\n",
    "    lof = LocalOutlierFactor(n_neighbors=n)\n",
    "    data = dataset[columns]\n",
    "    outliers = lof.fit_predict(data)\n",
    "    X_scores = lof.negative_outlier_factor_\n",
    "\n",
    "    dataset[\"outlier_lof\"] = outliers == -1\n",
    "    return dataset, outliers, X_scores\n",
    "\n",
    "# Loop over all columns\n",
    "dataset,outliers,X_scores = mark_outliers_lof(df,outlier_columns)\n",
    "for col in outlier_columns:\n",
    "    plot_binary_outliers(dataset,col,\"outlier_lof\",reset_index=True)\n",
    "# --------------------------------------------------------------\n",
    "# Check outliers grouped by label\n",
    "# --------------------------------------------------------------\n",
    "label = \"squat\"\n",
    "for col in outlier_columns:\n",
    "    dataset = mark_outliers_iqr(df[df['label']==label],col)\n",
    "    plot_binary_outliers(dataset,col,col+\"_outlier\",reset_index=True)\n",
    "\n",
    "for col in outlier_columns:\n",
    "    dataset = mark_outliers_chauvenet(df[df['label']==label],col)\n",
    "    plot_binary_outliers(dataset,col,col+\"_outlier\",reset_index=True)\n",
    "\n",
    "dataset,outliers,X_scores = mark_outliers_lof(df[df['label']==label],outlier_columns)\n",
    "for col in outlier_columns:\n",
    "    plot_binary_outliers(dataset,col,\"outlier_lof\",reset_index=True)\n",
    "# --------------------------------------------------------------\n",
    "# Choose method and deal with outliers\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# Test on single column\n",
    "\n",
    "\n",
    "# Create a loop\n",
    "outlier_removed_df = df.copy()\n",
    "\n",
    "for col in outlier_columns:\n",
    "    for label in df['label'].unique():\n",
    "        dataset = mark_outliers_chauvenet(df[df['label']==label],col)\n",
    "\n",
    "        dataset.loc[dataset[col+\"_outlier\"],col] = np.nan\n",
    "        outlier_removed_df.loc[(outlier_removed_df['label']==label),col]=dataset[col]\n",
    "\n",
    "        n_outliers = len(dataset)-len(dataset[col].dropna())\n",
    "        print(f\"removed {n_outliers} from {col} for {label}\")\n",
    "\n",
    "\n",
    "outlier_removed_df.info()\n",
    "# --------------------------------------------------------------\n",
    "# Export new dataframe\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "outlier_removed_df.to_pickle(\"data/interim/02_outliers_removed_chavnets.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7bea9-2541-49de-a960-3a8baa4d3ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#                                                            #\n",
    "#    Mark Hoogendoorn and Burkhardt Funk (2017)              #\n",
    "#    Machine Learning for the Quantified Self                #\n",
    "#    Springer                                                #\n",
    "#    Chapter 3                                               #\n",
    "#                                                            #\n",
    "##############################################################\n",
    "\n",
    "# Updated by Dave Ebbelaar on 22-12-2022\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.signal import butter, lfilter, filtfilt\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "# This class removes the high frequency data (that might be considered noise) from the data.\n",
    "# We can only apply this when we do not have missing values (i.e. NaN).\n",
    "class LowPassFilter:\n",
    "    def low_pass_filter(\n",
    "        self,\n",
    "        data_table,\n",
    "        col,\n",
    "        sampling_frequency,\n",
    "        cutoff_frequency,\n",
    "        order=5,\n",
    "        phase_shift=True,\n",
    "    ):\n",
    "        # http://stackoverflow.com/questions/12093594/how-to-implement-band-pass-butterworth-filter-with-scipy-signal-butter\n",
    "        # Cutoff frequencies are expressed as the fraction of the Nyquist frequency, which is half the sampling frequency\n",
    "        nyq = 0.5 * sampling_frequency\n",
    "        cut = cutoff_frequency / nyq\n",
    "\n",
    "        b, a = butter(order, cut, btype=\"low\", output=\"ba\", analog=False)\n",
    "        if phase_shift:\n",
    "            data_table[col + \"_lowpass\"] = filtfilt(b, a, data_table[col])\n",
    "        else:\n",
    "            data_table[col + \"_lowpass\"] = lfilter(b, a, data_table[col])\n",
    "        return data_table\n",
    "\n",
    "\n",
    "# Class for Principal Component Analysis. We can only apply this when we do not have missing values (i.e. NaN).\n",
    "# For this we have to impute these first, be aware of this.\n",
    "class PrincipalComponentAnalysis:\n",
    "\n",
    "    pca = []\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pca = []\n",
    "\n",
    "    def normalize_dataset(self, data_table, columns):\n",
    "        dt_norm = copy.deepcopy(data_table)\n",
    "        for col in columns:\n",
    "            dt_norm[col] = (data_table[col] - data_table[col].mean()) / (\n",
    "                data_table[col].max()\n",
    "                - data_table[col].min()\n",
    "                # data_table[col].std()\n",
    "            )\n",
    "        return dt_norm\n",
    "\n",
    "    # Perform the PCA on the selected columns and return the explained variance.\n",
    "    def determine_pc_explained_variance(self, data_table, cols):\n",
    "\n",
    "        # Normalize the data first.\n",
    "        dt_norm = self.normalize_dataset(data_table, cols)\n",
    "\n",
    "        # perform the PCA.\n",
    "        self.pca = PCA(n_components=len(cols))\n",
    "        self.pca.fit(dt_norm[cols])\n",
    "        # And return the explained variances.\n",
    "        return self.pca.explained_variance_ratio_\n",
    "\n",
    "    # Apply a PCA given the number of components we have selected.\n",
    "    # We add new pca columns.\n",
    "    def apply_pca(self, data_table, cols, number_comp):\n",
    "\n",
    "        # Normalize the data first.\n",
    "        dt_norm = self.normalize_dataset(data_table, cols)\n",
    "\n",
    "        # perform the PCA.\n",
    "        self.pca = PCA(n_components=number_comp)\n",
    "        self.pca.fit(dt_norm[cols])\n",
    "\n",
    "        # Transform our old values.\n",
    "        new_values = self.pca.transform(dt_norm[cols])\n",
    "\n",
    "        # And add the new ones:\n",
    "        for comp in range(0, number_comp):\n",
    "            data_table[\"pca_\" + str(comp + 1)] = new_values[:, comp]\n",
    "\n",
    "        return data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce597d-87aa-4719-95bd-bb965982b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "#                                                            #\n",
    "#    Mark Hoogendoorn and Burkhardt Funk (2017)              #\n",
    "#    Machine Learning for the Quantified Self                #\n",
    "#    Springer                                                #\n",
    "#    Chapter 4                                               #\n",
    "#                                                            #\n",
    "##############################################################\n",
    "\n",
    "# Updated by Dave Ebbelaar on 22-12-2022\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Class to abstract a history of numerical values we can use as an attribute.\n",
    "class NumericalAbstraction:\n",
    "\n",
    "    # This function aggregates a list of values using the specified aggregation\n",
    "    # function (which can be 'mean', 'max', 'min', 'median', 'std')\n",
    "    def aggregate_value(self, aggregation_function):\n",
    "        # Compute the values and return the result.\n",
    "        if aggregation_function == \"mean\":\n",
    "            return np.mean\n",
    "        elif aggregation_function == \"max\":\n",
    "            return np.max\n",
    "        elif aggregation_function == \"min\":\n",
    "            return np.min\n",
    "        elif aggregation_function == \"median\":\n",
    "            return np.median\n",
    "        elif aggregation_function == \"std\":\n",
    "            return np.std\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    # Abstract numerical columns specified given a window size (i.e. the number of time points from\n",
    "    # the past considered) and an aggregation function.\n",
    "    def abstract_numerical(self, data_table, cols, window_size, aggregation_function):\n",
    "\n",
    "        # Create new columns for the temporal data, pass over the dataset and compute values\n",
    "        for col in cols:\n",
    "            data_table[\n",
    "                col + \"_temp_\" + aggregation_function + \"_ws_\" + str(window_size)\n",
    "            ] = (\n",
    "                data_table[col]\n",
    "                .rolling(window_size)\n",
    "                .apply(self.aggregate_value(aggregation_function))\n",
    "            )\n",
    "\n",
    "        return data_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
